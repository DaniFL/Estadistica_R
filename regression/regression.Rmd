---
title: "Regression is all you need"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Antes de empezar...
Instala las librerías necesarias (copia y pega en la terminal; no descomentes
la línea)...

```{r}
 install.packages(
  c("easystats", "GGally", "qqplotr")
 )
```

... y carga las librerías más usadas: 

```{r, message=FALSE, warning=FALSE}
library("easystats")
library("tidyverse")
library("readr")
theme_set(theme_bw())  # cambia el tema de ggplot
```

El modelado estadístico es difícil, por lo que en este notebook solo 
cubriremos algunos aspectos básicos. Si en el futuro te enfrentas a experimentos
complejos, considera buscar ayuda.

>To consult the statistician after an experiment is finished is often merely to 
ask him to conduct a post mortem examination. He can perhaps say what the 
experiment died of. (Ronald Fisher)

## Modelado estadístico: las dos culturas
En el artículo clásico [Statistical modeling: The two cultures]((https://projecteuclid.org/journals/statistical-science/volume-16/issue-3/Statistical-Modeling--The-Two-Cultures-with-comments-and-a/10.1214/ss/1009213726.full)) encontramos 

> There are two cultures in the use of statistical modeling to reach conclusions from
data. One assumes that the data are generated by a given stochastic data model. The
other uses algorithmic models and treats the data mechanism as unknown. (Leo Breiman)

* *Explanatory modeling*: modelos estadísticos empleados para probar una teoría.
Para ello se emplean variables con un significado científico claro y se evalúa si tienen 
una relación *significativa* con la variable de interés. $\rightarrow$ "Estadística clásica"
basada en **modelos lineales**.
* *Predictive modeling*: modelos cuyo propósito es predecir observaciones nuevas 
o futuras. La interpretación de los modelos es secundaria. 
Gran parte del $\rightarrow$ **Aprendizaje automático** (*machine learning*) se 
basa en esta perspectiva.


## Regresión simple

El modelo básico sobre el que se construye gran parte de la estadística
es el modelo de **regresión lineal**:
$$y = a + b\cdot x + \epsilon$$
donde $\epsilon \sim \mathcal{N}(0, \sigma^2)$.

Es instructivo simular datos que sigan este modelo para entender el significado
de la ecuación. Para facilitar la interpretación, asumamos que estamos estudiando
la altura de los niños (de 0 a 12 años) en función de su edad:

$$altura = a + b\cdot edad + \epsilon \qquad \qquad \text{(altura en cm y edad en años)}$$

```{r}
age = seq(0, 12, 0.1)

# No todos los niños de la misma edad miden lo mismo. El término a + b * edad
# debe interpretarse como una altura media...
mean_height = 50 + 6.5 * age 
# ... en torno a la cuál habrá fluctuaciones estadísticas que podemos asumir normales.
epsilon = rnorm(length(mean_height), sd = 5)
height = mean_height + epsilon

df = data.frame(
  'age' = age,
  'height' = height, 
  'mean_height' = mean_height
)
library("ggplot2")
ggplot(df, aes(x = age, y = height)) + 
  geom_point() + 
  geom_line(aes(y = mean_height), col = 2)

# OR...
# plot(x, y)
# lines(x, expected_behaviour, col = 2)
```

En general, en una regresión lineal 
$$y = a + b\cdot x + \epsilon$$
la  relación entre $x$ e $y$ es muy específica. Si aumenta (disminuye) $x$ aumenta
(disminuye) $y$. El hecho de que $y$ aumente o disminuye en función de $x$ depende 
del signo de $a$. Además, un  incremento de una unidad en $x$ siempre produce el 
mismo incremento en $y$ (ídem si $x$ disminuye). Se dice que la relación entre 
$x$ e $y$ es **lineal**.

La primera pregunta a la que nos enfrentamos es la siguiente. 
**Dados los datos $(x, y)$, ¿podemos estimar el `comportamiento medio` (`expected age` en nuestro ejemplo)?**

![](https://media.giphy.com/media/l0ErOholJjSmFlMFG/giphy.gif)

### Ejemplo: linear model (lm)
```{r}
# 1) crear un modelo lineal... Interpretaremos la fórmula del modelo líneal como
# una forma de preguntar se "la altura depende de la edad".
naive_model = lm(height ~ age, data = df)
# 2) obtener estimaciones de a y b
summary(naive_model)
# 3) Obtener predicciones del modelo lineal
preds = predict(naive_model, interval = "confidence")
# 4) visualizar el ajuste
data_and_preds = bind_cols(df, preds)
ggplot(data_and_preds, aes(x = age)) + 
  geom_point(aes(y = height)) + 
  geom_line(aes(y = mean_height, col = "Expected")) + 
  geom_line(aes(y = fit, col = "Predicted")) + 
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.2, fill = "black")
```

Dado que las estimaciones incorporan el error de estimación, es posible realizar
inferencia acerca de la **significación** de los parámetros.


### Ejercicio: ggplot y lm
Si solo nos interesa la visualización y no los valores de los coeficientes podemos
emplear `geom_smooth(method = "lm")` para pintar la recta de regresión. Aplícalo a los
datos anteriores.

```{r}
ggplot(df, aes(x = age, y = height)) + 
  geom_point()+
  geom_smooth(method = "lm") # "lm" es el método de ajuste lineal
```


### Ejemplo: inferencia con lm
Un estudiante de biología desea determinar la relación entre
temperatura ambiente y frecuencia cardíaca en la rana leopardo, *Rana pipiens*.
Para ello, manipula la temperatura en incrementos de 2ºC que van desde
2ºC a 18ºC, registrando la frecuencia cardíaca (pulsaciones por minuto) en cada
intervalo. Los datos están disponibles en "hr.csv".

```{r}
# 1) leer los datos
library(readr)
hr <- read_table("regression/data/hr.csv")
View(hr)
# 2) Crear un modelo lineal
frog_model = lm(heart_rate ~ temperature, data = hr)

# 3) Inferencia
summary(frog_model)

ggplot(hr, aes(x = temperature, y = heart_rate)) + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  labs(title = "Frecuencia cardíaca en función de la temperatura",
       subtitle = "Datos de la rana leopardo, Rana pipiens",
       x = "Temperatura (ºC)",
       y = "Frecuencia cardíaca (pulsaciones por minuto)")

# Heart rate =  2.1389 + 1.7750 * temperature + N(0, sd = 2.639)

# La temperatura influye en la frecuencia cardíaca de la rana, de manera positiva, ya que por cada grado que aumenta la temperatura, 
# la frecuencia cardíaca aumenta 1.7750 pulsaciones por minuto. (P_valor 1.63e-05 ***) --> muy bajo, lo que confirma la relacion entre ambas variables
```

---

El diseño experimental y los resultados de la inferencia en el ejemplo de las ranas
nos invitan a concluir que "el aumento de la temperatura *causa* un incremento de 
la frecuencia cardíaca". Aunque en este caso esto es probablemente correcto, en general, 
esto no es así. Por muy fuerte que  parezca la relación entre las variables $x$ e $y$,
**no debemos interpretar  una variable como la causa de la otra**. Una relación
significativa entre $x$ e $y$ puede ocurrir por varios motivos:

1. $x$ causa $y$.
2. $y$ causa $x$.
3. Existe un tercer factor (llamado **variable de confusión**) que, bien directa
o indirectamente, causa $x$ e $y$.

![](https://qph.fs.quoracdn.net/main-qimg-13d22f6fda3811a9108d18b71c46e933-pjlq)

Para poder hacer afirmaciones acerca de la causalidad necesitamos muchos detalles
acerca del **diseño experimental**. En general, en nuestros ejercicios no dispondremos
de estos datos, por lo que lo mejor es ser cautelosos.

Por otra parte, respecto a los **p-valores**...

> There is some debate among statisticians and researchers about the appropriateness 
of P values, and that the term "statistical significance" can be misleading. If you 
have a small P value, it only means that the effect being tested is unlikely to be
explained by chance variation alone, in the context of the current study and the 
current statistical model underlying the test. If you have a large P value, it 
only means that the observed effect could plausibly be due to chance alone: it
is wrong to conclude that there is no effect (emmeans package authors)

En general, hay consenso en que 
**debe abandonarse la interpretación dicotómica del p-valor** 
(efecto significativo Vs no-significativo, sobre todo teniendo en 
cuenta que se basan en un threshold arbitrario) y 
**que debe favorecerse los resultados basados en intervalos de confianza y tamaños de los efectos**
(¡incluso si no son significativos!)

> For example, a study on the effects of two different ambient temperatures on 
paramecium diameter returning an effect size of 20 µm and a p-value of 0.1, 
if centred on p-value interpretation would conclude 'no effect' of temperature,
despite the best supported effect size being 20, not 0. An interpretation based on effect size and confidence intervals could, for example, state: 'Our results suggest that 
paramecium kept at the lower temperature will be on average 20 µm larger in size, 
however a difference in size ranging between −4 and 50 µm is also reasonably likely'. 
(...), the latter approach acknowledges the uncertainty in the estimated effect 
size while also ensuring that you do not make a false claim either of no effect
if p > 0.05, or an overly confident claim. (Lewis G. Halsey, [The reign of p-value is over](https://royalsocietypublishing.org/doi/10.1098/rsbl.2019.0174))

### Ejemplo: intervalos de confianza en lm
```{r}
summary(frog_model)
confint(frog_model)
```
Los resultados sugieren que el ritmo_cardiaco de las ranas se incrementará
1.77 bpms por cada grado Celsius, si bien un aumento de la frecuencia 
cardiaca en el rango (1.37, 2.17) es igualmente verosímil.

### Validación de los modelos
Evidentemente cualquier interpretación está supeditada a que el modelo sea 
correcto. Debemos ser muy cuidadosos a la hora de verificar que se cumplan las
asunciones del modelo de regresión lineal. Podemos usar el acrónimo **LINE** para
recordar las asunciones más importantes del modelo: 
**Linear, Independent, Normal, Equal variances**.

### Ejemplo: evaluación del modelo `naive_model`
```{r}
# Lo primero siempre debe ser pintar las predicciones (por ejemplo, con geom_smooth)
# para ver si se ajustan de forma razonable a los datos. Como ya lo hemos hecho, 
# lo saltamos...

# Para QQplot también nos apoyaremos en check_normality
plot(naive_model, ask = FALSE) 

# Comprobar la normalidad con qqplot puede ser difícil. Podemos apoyarnos en 
# performance::check_normality (librería easystats)

# check_normality corre shapiro.test, pero tal y como resalta la documentación
# "this formal test almost always yields significant results for the distribution
# of residuals and visual inspection (e.g. Q-Q plots) are preferable."
is_norm = check_normality(naive_model)

# Para hacer la inspección visual
plot(is_norm, type = "qq", detrend = TRUE) # probar con TRUE y FALSE
```

## Transformaciones logarítmicas
### Ejemplo: estudio y rendimiento
Se desea estudiar la relación entre el tiempo de estudio y el rendimiento en 
un examen particular. Datos en "exam_scores.csv".

```{r}
scores = read_csv("data/exam_scores.csv")
ggplot(scores, aes(x = study_time_hours, y = exam_score)) + 
  geom_point() + 
  geom_smooth(method = "lm")
# Grafica que los efectos an disminuyendo
```

Nótese que un aumento en el tiempo de estudio tiene un impacto sustancial en
el rendimiento inicial, pero este impacto disminuye a medida que aumenta el 
tiempo de estudio. Lo más reseñable para nuestro análisis es que la relación entre
ambas variables es ¡NO-LINEAL! 

**Las transformaciones de variables nos pueden ayudar en estos casos**. La más típica
es la **transformación logarítmica, que  ayuda a linearizar relaciones**. Algunos 
casos habituales donde se suele emplear el logaritmo son:

* Con variables donde se espera un efecto multiplicativo y/o rendimientos crecientes/decrecientes.
* Con variables donde es más relevante el orden de magnitud que el valor numérico preciso.

```{r}
ggplot(scores, aes(x = log(study_time_hours), y = exam_score)) + 
  geom_point() + 
  geom_smooth(method = "lm")

# o mejor...
ggplot(scores, aes(x = study_time_hours, y = exam_score)) + 
  geom_point() + 
  coord_trans(x = "log")
```

```{r}
study_model = lm(exam_score ~ log(study_time_hours), scores)
summary(study_model)
```

¡Ojo! Con este truco, cambia la interpretación de los coeficientes de la regresión.
Usa `log_interpretation` de `utils.R`:

```{r}
suppressPackageStartupMessages(
  source("utils.R")
)
log_interpretation(20.2753, "predictor", percent_increase = 20)
# percent increase sacado de summary de log studytimehours
# Both : log(x) log(y)
# response : log(y)
# predictor: log(x)
```


### Ejercicio: ¿Influye la altura en los salarios?
Quizás hayas escuchado que la gente más alta gana más. ¿Es cierto? Usa los datos
en "height_earnings.csv" para indigar sobre esto (datos procedentes de una encuesta
sobre trabajo, familia y bienestar de EEUU; las alturas están en pulgadas y las 
ganancias son ganancias anuales en dolares). Usa la transformación logarítmica
sobre las ganancias para centrarte en el orden de magnitud.

```{r}
# Resultado esperado: cada cm extra parece incrementar el salario en 4.36%
# Usamos la transformacion logaritmica para usar regresion lineal

library(readr)
df <- read_csv("regression/data/heights_earnings.csv")
View(heights_earnings)

ha_model = lm(log(earn) ~ height, df)

library(ggplot2)
ggplot(df, aes(x = height, y = earn)) + 
  geom_jitter(width = 0.5, alpha = 0.5) + 
  geom_smooth(method = "lm")

df$predictions = predict(ha_model)
summary(ha_model)

# ---------------------------------------------------- #

df = read.csv("data/heights_earnings.csv")
ggplot(df, aes(x = height, y = earn)) + 
  geom_jitter(width = 0.5, alpha = 0.4) + 
  geom_smooth(method = "lm")
# Parece apoyar que los salarios aumentan segun la altura
# Parece que esta discretizado la altura
# Dificulta trazar una linea recta ya que hay pocos puntitos
# usaremos algo distinto a geom_point()
# geom_jitter añade un desplazamiento aleatorio del eje X suficientemente pequeño
# como para calcular una mejor linea

ha_model = lm(log(earn) ~ height, df)
```


Las conclusiones de este ejercicio seguramente no sean correctas, ya que estamos
intentando explicar el salario empleando un solo factor (altura) cuando en realidad
el salario está determinado por múltiples factores. ¿Se te ocurre algún ejemplo 
de por qué nuestras conclusiones pueden estar sesgadas? (Intenta pensar en un colectivo
que suele ganar menos dinero y que también suele tener alturas menores.)


## Regresión múltiple
Para lidiar con situaciones como la ilustrada en el gráfico de 
"correlation is not causation" (tiburones Vs helados)
necesitamos emplear modelos de **regresión múltiple**, dado que estos permiten 
"controlar" las variables de confusión. Crear un modelo de regresión múltiple
es análogo al caso unidimensional...

### Ejemplo: ¿Qué influye en los salarios?
Añade la edad a tu modelo de los salarios para mejorar las predicciones.

```{r}
# Si meto sexo al modelo al ser un factor lo interpreta como un factor no como una variable continua
# log(earn) = 8.59 + 0.018 * height + 0.2566177 * sexmale(variable binaria)
# Ecuacion para hombres 
# log(earn) = 8.59 + 0.018 * height + 0.2566177 
# Ecuacion para mujeres
# log(earn) = 8.59 + 0.018 * height

# A igualdad de alturas el hombre siempre gana 0.25 más.

library("GGally")

df <- read_csv("data/heights_earnings.csv")
ggpairs(df[, c("earn", "height", "age")])

ggplot(df, aes(x = height, y = earn, col = sex)) + 
  geom_jitter(width = 0.5, alpha = 0.5) + 
  geom_smooth(method = "lm")

ha_model = lm(log(earn) ~ height + age, df)
df$predictions = predict(ha_model)

summary(ha_model)

source("utils.R")
log_interpretation(coefficient = 0.018561,
                   log_transformation = "response"
)
log_interpretation(coefficient = 0.266177,
                   log_transformation = "response"
)

```

### Ejercicio: asunciones del modelo de los salarios

```{r}
# ??
```

---

Hasta ahora solo hemos usado datos continuos, pero nada evita **usar datos categóricos
como predictores **. ¡Ojo! Los coeficientes asociados a datos categóricos** no deben 
interpretarse como una pendiente**.

### Ejemplo: regresión múltiple con datos categóricos
Construye un modelo de regresión lineal para predecir el peso de una persona a partir
de los datos contenidos en "antrop.csv". Interpreta los coeficientes de la regresión.

```{r}
# Ojo con la columna male! ¿Por qué?
## ??????

# antrop_model = lm(?????, data = antrop)

# Chequea las asunciones del modelo.
# ???

# Descomenta las siguientes líneas tras completar las líneas anteriores
# antrop_preds = bind_cols(antrop, fit = predict(antrop_model))
# ggplot(antrop_preds, aes(x = height, col=male)) + 
#   geom_point(aes(y = weight)) + 
#   geom_line(aes(y = fit), lwd = 3)

# summary(antrop_model)

antrop = read.csv("data/antrop.csv")

antrop$male = factor(antrop$male)
# Ojo con la columna male! ¿Por qué?
# es un factor por lo que es una variable binaria y no continua

ggplot(antrop, aes(x = weight, y = height, col = male)) + geom_point()

# Esta okay datos dispersos tiene buena pinta no clusters rarete male no es 0 o 1, añado smooth

ggplot(antrop, aes(x = weight, y = height, col = male)) + 
  geom_point() + 
  geom_smooth()
# overfit un poco

ggplot(antrop, aes(x = weight, y = height, col = male)) + 
  geom_point() + 
  geom_smooth(method = "lm")

# Vamos a crear el modelo

antrop_model = lm(weight ~ height + male, data = antrop)
summary(antrop_model)
# Chequea las asunciones del modelo.
# Media negativa deberia tener en cuenta male?

# Informacion sobre el modelo
summary(antrop_model)

# PREGUNTAR A TINO
# Si le sumo sex si que me dan los valores que ha puesto del modelo
# Pudiendo sacar la formula $$weight = -29 + 0.47 * height + 1.23 * male$$
# Conclusiones abajo
```

El modelo se puede escribir como
$$weight = -29 + 0.47 * height + 1.23 * male$$ 
por lo que 1.23 significa que, de media y para una misma altura, los hombres 
pesan 1.23 Kg más que las mujeres (hemos **ajustado por el efecto de la altura**).
Esto es muy importante: **a la hora de interpretar un coeficiente en regresión múltiple
lo hacemos asumiendo "igualdad de condiciones" en los otros coeficientes.**

### Ejercicio: Intervalos de confianza
Usa intervalos de confianza para interpretar los resultados de la regresión.

```{r}
# Calcula intervalos de confianza para los coeficientes
conf_interval = confint(antrop_model)

conf_interval

# Ver si el modelo es valido
```

### Ejercicio: Howell
Los datos contenidos en "howell1.csv" son datos censales parciales del 
área !Kung San compilados a partir de entrevistas realizadas a finales de la década
de 1960. Crea un modelo para predecir el peso de los individuos a partir 
de la altura y el sexo. Evalúa la bondad del modelo.

```{r}
# ??
```

Sin ni siquiera usar `plot(howell_model)` ya somos capaces de ver que el ajuste
es malo... cualquier conclusión basada en un modelo erróneo será errónea 
(**garbage in, garbage out**).


### Ejemplo: dummy variables y contrastes
El dataset `iris` (puedes obtenerlo con `data(iris)`) contiene medidas del sépalo
y pétalo de varias especies de iris. Construye un modelo lineal para predecir
la longitud del sépalo únicamente en función de la especie. Interpreta los coeficientes
de la regresión.

```{r}
data(iris)
iris_model = lm(Sepal.Length ~ Species, iris)
print(summary(iris_model))

iris_preds = bind_cols(iris, fit = predict(iris_model))
ggplot(iris_preds, aes(x=Species, fill = Species)) + 
  geom_boxplot(aes(y=Sepal.Length)) + 
  geom_point(aes(y = fit), shape=4, size=3)
```

Al interpretar los coeficientes de la regresión, observamos que 
`lm` ha tomado como referencia la especie `setosa`. Esto se puede observar usando
`contrasts`.

```{r}
contrasts(iris$Species)
```

Es decir el modelo es
$$sepal = \text{mean-setosa-sepal} + 0.93 * versicolor + 1.58 * virginica.$$
```{r}
sepal = 5.00 + 0.93 * versicolor + 1.58 * virginica
# setosa (especie de referencia)
sepal = 5.00
# versicolor:
sepal = 5.00 + 0.93 = 5.93

# 0.93 es el salto que me dice que mide 0.93 mas en media que setosa
# virginica
sepal = 5.00 + 1.58 = 6.58

```

Sin embargo, podríamos reescribir el modelo de otra forma de forma 
que los coeficientes tengan otro significado. Un ejemplo sencillo sería:
$$sepal = \text{mean-versicolor-sepal} + \alpha_1 * setosa + \alpha_2 * virginica.$$

En este caso, simplimente estamos variando la especie de referencia. De hecho, 
`contrasts` se puede modificar para usar como referencia otro nivel del factor:

### Ejemplo: contrastes
```{r}
# contrasts(iris$Species)
# El output es que la variable de referencia tiene las dos variables a 0
# relevel(iris$Species, versicolor")
# si ejecutamos eso versicolor ahora es la variable de referencia

iris$Species = relevel(iris$Species, "versicolor")
contrasts(iris$Species)
iris_model_2 = lm(Sepal.Length ~ Species, iris)
print(summary(iris_model_2))

# Las interpretaciones se apoyan en la numeracion
# AHora cambian los numero vemos las medias de virginica  y setosa cambiadas
# versicolor es mi referencia y lo comparo con los otros dos
# Solo se pueden elegir n-1 preguntas y hay que elegirlos bien para no hacer un modelo para todo
# Las preguntas son los contrastes y son limitados, repetir modelos hasta generar todas las posibilidades es mala idea
```

Lo interesante es que podemos 
**ajustar los contrastes de forma que respondan a nuestras preguntas científicas**. 
En general, estos contrastes deben ser **ortogonales**.

### Ejemplo: contrastes ortogonales
Imagínemonos el siguiente universo paralelo. En este universo paralelo solo 
existe la especie setosa. Una empresa de ingeniería genética te contrata para
crear nuevas especies con un sépalo más grande. Desarrollas un método conocido
como "Método V", que tiene dos variantes "V-I" y "V-II". Los experimentos con
estas variantes dan lugar a dos nuevas especies que llamas versicolor (V-I) y 
virginica (V-II). Te planteas dos preguntas científicas: 

1. ¿Es el método V capaz de crear especies con el sépalo más grande?
2. ¿Existe alguna diferencia entre V-I y V-II?

```{r}
suppressPackageStartupMessages(
  source("utils.R")   # cargamos get_contrasts_coding
)
# Cambiamos otra vez la especie referencia (no es necesario, pero para tener 
# a las vs juntas)
iris$Species = relevel(iris$Species, "setosa")
levels(iris$Species)
contrasts(iris$Species)

# Pego las filas para ponerlo todo en una matriz
# H0: 0.5 * versicolor + 0.5 * virginica - mu_setosa = 0
# H0: mu_versicolor - mu_virginica = 0
my_contrasts = rbind(
  "V - setosa" = c(-1, 0.5, 0.5), 
  # Veo porque numero esta multiplicado la variable
  "I - II" = c(0, 1, -1)
)
# Una vez codificado los contrastes usar get_contrasts de utils para las filas
my_coding = get_contrasts_coding(my_contrasts)
contrasts(iris$Species) = my_coding
contrasts(iris$Species)

v_model = lm(Sepal.Length ~ Species, iris)
summary(v_model)
confint(v_model)
```

El método V parace producir sépalos más grandes. Por otra parte, V-II es mejor
que V-I.

---

LLegados a este punto, ¡ya hemos cubierto el 90% de los contenidos habituales 
de un curso de estadística habitual! Aunque parezca mentira, ya hemos hecho 
análisis tan complejos como

* Análisis de la varianza (anova): por ejemplo, en el problema del método V,
* Análisis de la covarianza (ancova): con el dataset `antrop.csv` o `howell`,
* ...

Desde una perspectiva moderna, 
**todos los análisis clásicos (T-test, anova, ancova) pueden considerarse como simples modelos de regresión**.
Esto demuestra el  poder unificador de esta perspectiva. En las siguientes secciones
revisaremos sin embargo estos modelos clásicos para afianzar la conexión con los
modelos de regresión.


### Ejercicio: ¿Qué influye en los salarios?
Añade la variable sexo al modelo de los salarios. ¿Cuál es la diferencia en ganancias 
para hombres y mujeres de la misma altura? 

```{r}
# ???
# Resultado esperado: a igualdad de condiciones, los hombres ganan un 30.4% más que las mujeres
```

Añade la variable educación al modelo de los salarios. Modifica los contrastes
para responder a las siguientes preguntas:

1. ¿Merece la pena estudiar "high school" o "universidad" o basta con quedarse en
"elementary" (en términos de salario)?
2. ¿Merece la pena estudiar "universidad" comparado con "high school"?

```{r}
# ??????
# Resultado esperado: sí merece la pena, con incrementos en las ganancias del 83.58% y 33.68%, respectivamente.
```

